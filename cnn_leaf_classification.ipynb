{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Major Task\n",
    "## CNN Leaf Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "  <li><a href=\"#overview\">Overview</a></li>\n",
    "  <li><a href=\"#part1\">Part I: Data Preparation</a>\n",
    "    <ol>\n",
    "      <li><a href=\"#lookatdata\">Describe the Data</a></li>\n",
    "      <li><a href=\"#clean-the-data\">Clean the Data</a></li>\n",
    "      <li><a href=\"#check-for-missing-values-and-duplicates\">Check for Missing Values and Duplicates</a></li>\n",
    "      <li><a href=\"#visualize-the-data\">Visualize the Data</a></li>\n",
    "      <li><a href=\"#draw-images\">Draw Images</a></li>\n",
    "      <li><a href=\"#correlation-analysis\">Correlation Analysis</a></li>\n",
    "      <li><a href=\"#divide-the-data\">Divide the Data</a></li>\n",
    "      <li><a href=\"#standardize-the-data\">Standardize the Data</a></li>\n",
    "      <li><a href=\"#encode-the-labels\">Encode the Labels</a></li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li><a href=\"#training-a-neural-network\">Part II: Training a Neural Network (CNN)</a>\n",
    "    <ol>\n",
    "      <li><a href=\"#implement-a-cnn-model\">Implement a CNN Model</a></li>\n",
    "      <li><a href=\"#write-training-function\">Write Training Function</a></li>\n",
    "      <li><a href=\"#explore-hyperparameter-settings\">Explore Hyperparameter Settings</a></li>\n",
    "      <li><a href=\"#tensorboard-monitoring\">TensorBoard Monitoring</a></li>\n",
    "      <li><a href=\"#evaluation-function\">Evaluation Function</a></li>\n",
    "    </ol>\n",
    "  </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Description</h3>\n",
    "<a id=\"description\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets write our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: Data Preparation\n",
    "<a id=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Taking a look and Describing the data</h2>\n",
    "<a id=\"lookatdata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "train_df = pd.read_csv(r'.\\data_files\\train.csv')\n",
    "\n",
    "print(\"#-----> First 5 rows of the training set:\\n\")\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> training set description:\")\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> training set information\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> training set value types\")\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing set\n",
    "test_df = pd.read_csv(r'.\\data_files\\test.csv')\n",
    "\n",
    "print(\"#-----> First 5 rows of the testing set:\")\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> testing set description:\")\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> testing set information\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> testing set value types\")\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning the data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the data for missing values or duplicates and carrying out proper correction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", train_df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Duplicate values:\\n\", train_df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----> Looks like we don't have any missing or duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue lets setup our data by dropping the the id and species from the features and set the target on species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'id' and 'species' columns\n",
    "X_features = train_df.drop(['id', 'species'], axis=1)\n",
    "y_target = train_df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distributions\n",
    "features = train_df.iloc[:, 2:]  # Assuming features start from column 2\n",
    "plt.figure(figsize=(24, 16))\n",
    "for i, feature in enumerate(features.columns, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(train_df[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualization 3: Pairwise Feature Scatter Plots\n",
    "# sns.pairplot(train_df.sample(500), hue='species', diag_kind='kde')\n",
    "# plt.suptitle('Pairwise Scatter Plots for Features', y=1.02)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Visualization (using PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(features)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=train_df['species'])\n",
    "plt.title('PCA Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's display some leaf images from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_dir = '.\\data_files\\images'\n",
    "image_ids = train_df['id'].head(5).tolist() \n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for i, image_id in enumerate(image_ids, 1):\n",
    "    image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    plt.subplot(1, 5, i)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Image {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Correleation Analysis </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to calculate the correlation matrix for shape features<br>\n",
    "we will use heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns related to 'margin' and 'texture'\n",
    "margin_texture_columns = train_df.loc[:, 'margin1':'texture64']\n",
    "\n",
    "# Calculate correlation matrix for 'margin' and 'texture' features\n",
    "correlation_matrix_margin_texture = margin_texture_columns.corr()\n",
    "\n",
    "# Display heatmap for the correlation matrix\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(correlation_matrix_margin_texture, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for Margin and Texture Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding which split method to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We got two methods for splitting:\n",
    "<ol>\n",
    "<li>train_test_split</li>\n",
    "<li>StratifiedShuffleSplit (sss)</li>\n",
    "</ol>\n",
    "\n",
    "<b>train_test_split:</b></br>\n",
    "Usage: Commonly used for general train-test splitting, especially when the class distribution is not a significant concern.<br>\n",
    "How it works: Randomly shuffles and splits the data into training and test sets.<br>\n",
    "Advantage: Simplicity and ease of use. Suitable for well-balanced datasets.<br>\n",
    "\n",
    "<b>StratifiedShuffleSplit:</b></br>\n",
    "Usage: Typically used when you want to ensure that the distribution of classes in both the training and validation sets is representative of the overall distribution in the dataset.<br>\n",
    "How it works: StratifiedShuffleSplit maintains the class distribution when creating random splits. It shuffles the data and then creates splits, ensuring that each split has a similar class distribution.<br>\n",
    "Advantage: Useful when dealing with imbalanced datasets where certain classes have significantly fewer samples than others.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset has a <b>balanced</b> class distribution, and just need a simple split, train_test_split is often sufficient and easier to use.<br>\n",
    "\n",
    "If the dataset has <b>imbalanced</b> classes, and want to ensure that the class distribution is maintained in both training and validation sets, then StratifiedShuffleSplit is a good choice.<br>\n",
    "\n",
    "To decide which approach is better the dataset, we can can check the distribution of the 'species' column in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(x='species', data=train_df)\n",
    "plt.title('Distribution of Leaf Classes')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----> since all the bars are the same height that means its balanced and we can use the regular train_test_split method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train/Test split</h2>\n",
    "Divide the data into a training and testing set using approximately 80% for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size = 0.2 meaning that the training set will be 0.8 (80%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Standardization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Label Encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# read image\n",
    "img = cv2.imread('data_files/images/1.jpg')\n",
    "color = (0,0,0)\n",
    "result = img.copy()\n",
    "result2b = cv2.copyMakeBorder(result, 0,0,90,90, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(img)\n",
    "cv2.imshow(\"result2b\", result2b)\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(result2b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('/data_files/images/leaf-classification/images.zip') as z_img:\n",
    "#     z_img.extractall()\n",
    "from PIL import Image, ImageOps\n",
    "import glob\n",
    "image_list = []\n",
    "for filename in glob.glob('data_files/images/*.jpg'): #assuming jpg\n",
    "    # im=Image.open(filename)\n",
    "    img = cv2.imread(filename)\n",
    "    dimensions = img.shape\n",
    " \n",
    "    # height, width, number of channels in image\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    diff = abs(width-height)\n",
    "    color = (0,0,0)\n",
    "    result = img.copy()\n",
    "    if width<height:\n",
    "        result2b = cv2.copyMakeBorder(result, 0,0,diff,diff, cv2.BORDER_CONSTANT, value=color)\n",
    "    elif height>width:\n",
    "        result2b = cv2.copyMakeBorder(result, diff,diff,0,0, cv2.BORDER_CONSTANT, value=color)\n",
    "        \n",
    "    image_list.append(result2b)\n",
    "    \n",
    "plt.figure(figsize=(24, 16))\n",
    "for i in range(25):\n",
    "    # j=np.random.choice((os.listdir('images')))\n",
    "    plt.subplot(5,5,i+1)\n",
    "    # img=load_img(os.path.join('/kaggle/working/images',j))\n",
    "    img = image_list[i]\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part II: Training the Neural Network</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
